{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-03 19:11:13,271 - RelationshipScorerPackage - INFO - Logging initialized. Level: INFO. Outputting to console only.\n"
     ]
    }
   ],
   "source": [
    "from src.main_scorer import RelationshipScorer\n",
    "# Import specific exceptions for more granular error handling if needed\n",
    "from src.exceptions import InputValidationError, ConfigurationError, CalculationError, ScoringInitializationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare your input data dictionary\n",
    "# This structure MUST conform to the ScorerInputData Pydantic model.\n",
    "# See config/data_model_config.yaml for a detailed example.\n",
    "input_data = {\n",
    "    \"relationship_mentions\": [\n",
    "        {\"source_type\": \"Guideline\", \"year\": 2024, \"sentiment\": \"Positive\", \"mention_id\": \"guideline-abc\"},\n",
    "        {\"source_type\": \"Phase 3 CT\", \"year\": 2023, \"sentiment\": \"Positive\", \"mention_id\": \"NCT12345\"},\n",
    "        {\"source_type\": \"PubMed\", \"year\": 2022, \"sentiment\": \"Neutral\", \"mention_id\": \"pmid:30000001\"},\n",
    "        {\"source_type\": \"PubMed\", \"year\": 2021, \"sentiment\": \"Negative\", \"mention_id\": \"pmid:30000000\"},\n",
    "        {\"source_type\": \"Review\", \"year\": 2024, \"sentiment\": \"Positive\", \"mention_id\": \"review-xyz\"}\n",
    "        # Add all relevant mentions for the specific entity pair\n",
    "    ],\n",
    "    \"entity_a_metadata\": {\n",
    "        \"id\": \"DRUG_X\",\n",
    "        # This score must be pre-calculated based on your entire dataset\n",
    "        \"overall_prominence\": 250.5\n",
    "    },\n",
    "    \"entity_b_metadata\": {\n",
    "        \"id\": \"DISEASE_Y\",\n",
    "        # This score must be pre-calculated based on your entire dataset\n",
    "        \"overall_prominence\": 180.0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing scorer...\n",
      "2025-04-03 19:11:13,284 - RelationshipScorerPackage - INFO - Initializing RelationshipScorer for entity pair: DRUG_X - DISEASE_Y\n",
      "2025-04-03 19:11:13,284 - RelationshipScorerPackage - INFO - Configuration loaded successfully.\n",
      "2025-04-03 19:11:13,285 - RelationshipScorerPackage - INFO - RelationshipScorer initialized successfully.\n",
      "Scorer initialized.\n",
      "Calculating all scores...\n",
      "2025-04-03 19:11:13,285 - RelationshipScorerPackage - INFO - Calculating all ensemble scores for DRUG_X - DISEASE_Y...\n",
      "2025-04-03 19:11:13,285 - RelationshipScorerPackage - INFO - Calculating Evidence Strength score...\n",
      "2025-04-03 19:11:13,286 - RelationshipScorerPackage - INFO - Evidence Strength calculation complete. Score: 5.6814825873066335\n",
      "2025-04-03 19:11:13,286 - RelationshipScorerPackage - INFO - Calculating Sentiment scores...\n",
      "2025-04-03 19:11:13,287 - RelationshipScorerPackage - INFO - Sentiment score calculation complete. Dominant: Positive, Net: 16.50\n",
      "2025-04-03 19:11:13,287 - RelationshipScorerPackage - INFO - Calculating Trend score...\n",
      "2025-04-03 19:11:13,287 - RelationshipScorerPackage - INFO - Trend score calculation complete using method 'RecencyWeighted'. Score: 15.529490840694272\n",
      "2025-04-03 19:11:13,287 - RelationshipScorerPackage - INFO - Successfully calculated and validated all scores.\n",
      "\n",
      "--- All Scores ---\n",
      "{\n",
      "  \"evidence_strength\": 5.6814825873066335,\n",
      "  \"sentiment_scores\": {\n",
      "    \"positive_score\": 17.5,\n",
      "    \"negative_score\": 1.0,\n",
      "    \"neutral_score\": 1.0,\n",
      "    \"net_score\": 16.5,\n",
      "    \"dominant_sentiment\": \"Positive\"\n",
      "  },\n",
      "  \"trend_score\": 15.529490840694272\n",
      "}\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# 2. Initialize the scorer and calculate scores\n",
    "try:\n",
    "    # Initialization validates input using Pydantic and loads config from files\n",
    "    print(\"Initializing scorer...\")\n",
    "    scorer = RelationshipScorer(input_data=input_data)\n",
    "    print(\"Scorer initialized.\")\n",
    "\n",
    "    # Calculate all scores at once\n",
    "    print(\"Calculating all scores...\")\n",
    "    all_scores = scorer.get_all_scores()\n",
    "    print(\"\\n--- All Scores ---\")\n",
    "    # Pretty print the dictionary\n",
    "    import json\n",
    "    print(json.dumps(all_scores, indent=2))\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # Alternatively, calculate individual scores if needed\n",
    "    # print(\"\\nCalculating individual scores...\")\n",
    "    # evidence = scorer.get_evidence_strength()\n",
    "    # print(f\"Evidence Strength: {evidence:.3f}\")\n",
    "    #\n",
    "    # sentiment = scorer.get_sentiment_scores()\n",
    "    # print(\"Sentiment Scores:\", sentiment)\n",
    "    #\n",
    "    # trend = scorer.get_trend_score()\n",
    "    # print(f\"Trend Score: {trend:.3f}\")\n",
    "\n",
    "except InputValidationError as e:\n",
    "    print(f\"\\nERROR: Input data is invalid.\")\n",
    "    print(f\"Details: {e.details}\") # Pydantic validation error details\n",
    "except ConfigurationError as e:\n",
    "    print(f\"\\nERROR: Configuration problem.\")\n",
    "    print(f\"Details: {e}\")\n",
    "except CalculationError as e:\n",
    "    print(f\"\\nERROR: Problem during score calculation.\")\n",
    "    print(f\"Details: {e}\")\n",
    "except ScoringInitializationError as e:\n",
    "    print(f\"\\nERROR: Failed to initialize the scorer.\")\n",
    "    print(f\"Details: {e}\")\n",
    "except Exception as e:\n",
    "    # Catch any other unexpected errors\n",
    "    print(f\"\\nAn unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
